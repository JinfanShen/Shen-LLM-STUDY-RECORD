{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97ba1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "830a3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 9d886e36-2f03-442c-8012-3c8de228ac5e)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 081dffff-14f0-4a7b-8a92-09a1b6c045e7)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 0e77c559-1dd3-4ed8-a76f-631ab3e53631)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: be37988a-f407-4068-b851-1480c3b5d703)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: d9a28375-c534-405d-91ed-3ccb8874545a)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 8049caa0-d69e-435d-b7a3-e97bcde35485)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")  #加载Tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a7ffbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['要', '分', '析', '的', '文', '件', '和', '单', '元', '格', '是', '否', '都', '包', '含', '在', '代', '码', '块', '中', '？']\n",
      "[6206, 1146, 3358, 4638, 3152, 816, 1469, 1296, 1039, 3419, 3221, 1415, 6963, 1259, 1419, 1762, 807, 4772, 1779, 704, 8043]\n",
      "[6206, 1146, 3358, 4638, 3152, 816, 1469, 1296, 1039, 3419, 3221, 1415, 6963, 1259, 1419, 1762, 807, 4772, 1779, 704, 8043]\n"
     ]
    }
   ],
   "source": [
    "seq = '要分析的文件和单元格是否都包含在代码块中？'\n",
    "tokens = tokenizer.tokenize(seq)  #分词\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) #分词转为id\n",
    "print(tokens)\n",
    "print(ids)\n",
    "idxs = tokenizer.encode(seq,add_special_tokens=False)  #add_special_tokens会添加开始和结尾标记\n",
    "print(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d96a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6206, 1146, 3358, 4638, 3152, 816, 1469, 1296, 1039, 3419, 3221, 1415, 6963, 1259, 1419, 1762, 807, 4772, 1779, 704, 8043, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 6206, 1146, 3358, 4638, 3152, 816, 1469, 1296, 102]\n"
     ]
    }
   ],
   "source": [
    "#填充和截断\n",
    "ids = tokenizer.encode(seq, padding='max_length', max_length=30)  #填充到最大长度\n",
    "print(ids)\n",
    "ids = tokenizer.encode(seq, max_length=10, truncation=True)  #按最大长度截断\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3192ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [101, 6206, 1146, 3358, 4638, 3152, 816, 1469, 1296, 1039, 3419, 3221, 1415, 6963, 1259, 1419, 1762, 807, 4772, 1779, 704, 8043, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    seq,\n",
    "    padding = 'max_length',\n",
    "    max_length = 32,\n",
    "    add_special_tokens = True)\n",
    "for k, v in inputs.items():  #打印出来的attention_mask中0代表了该位置ids是padding\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd90c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[101, 872, 1962, 8024, 686, 4518, 8013, 102, 0], [101, 6821, 3221, 671, 702, 3844, 6407, 511, 102], [101, 2769, 1373, 2207, 4374, 511, 102, 0, 0]]\n",
      "token_type_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#处理Batch数据\n",
    "seqs = [\n",
    "    \"你好，世界！\",\n",
    "    \"这是一个测试。\",\n",
    "    \"我叫小王。\"\n",
    "]\n",
    "\n",
    "res = tokenizer(seqs, max_length=10, padding='longest', add_special_tokens=True)  #按batch最长填充处理\n",
    "for k,v in res.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce4930",
   "metadata": {},
   "source": [
    "## Fast/Slow Tokenizer\n",
    "Fast 通过Rust 实现，Slow 通过Python 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d818d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 57b84e29-b899-4340-9eb7-0b076c4a0bd9)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: fa303705-6394-416f-9e27-d6c0a23fac64)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 05ccd6b3-3597-4e84-a380-532d4e3455ca)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 1f3eef4d-4f41-49f5-bbb6-a437fb59aedb)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 88bee8ef-3b5c-47e8-8a2c-555b6cd1dea3)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\"), '(Request ID: 8f0f1dc8-13b0-4a5d-9df6-1b7e46a60f7b)')' thrown while requesting HEAD https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\",use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46a7d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ),\n",
       " BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " ))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, slow_tokenizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d13f2f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 422 ms\n",
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10000):\n",
    "    tokenizer(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a874d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.25 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
