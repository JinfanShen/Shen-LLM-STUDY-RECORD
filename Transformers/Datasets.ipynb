{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3af9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets  as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d9635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a3f2ae1b364a2ab8a7e9060942e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/22.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\vit\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fan\\.cache\\huggingface\\hub\\datasets--madao33--new-title-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d9564983ca43c7b01bc24708305e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cb0d10210d43d792d32b6fc94337b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3144882146422d8b2bc2a5a8b52763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = ds.load_dataset('madao33/new-title-chinese')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d482e",
   "metadata": {},
   "source": [
    "# 按照数据集划分进行加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7878435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5850\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindatasets =  ds.load_dataset('madao33/new-title-chinese', split='train')\n",
    "traindatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78162fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 2925\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindatasets = ds.load_dataset('madao33/new-title-chinese', split='train[:50%]') #加载前50%的数据\n",
    "traindatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd0076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['title', 'content'],\n",
       "     num_rows: 2925\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'content'],\n",
       "     num_rows: 4095\n",
       " })]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindatasets = ds.load_dataset('madao33/new-title-chinese', split=['train[:50%]', 'train[30%:]']) #分别加载前50%的数据, 后70%的数据\n",
    "traindatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41576954",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4abe306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ef011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['望海楼美国打“台湾牌”是危险的赌博',\n",
       " '大力推进高校治理能力建设',\n",
       " '坚持事业为上选贤任能',\n",
       " '“大朋友”的话儿记心头',\n",
       " '用好可持续发展这把“金钥匙”']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6595ac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'content']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694b5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': Value('string'), 'content': Value('string')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5d0cc",
   "metadata": {},
   "source": [
    "# 数据集的划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb34400a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 4680\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 1170\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindatasets = datasets['train']\n",
    "traindatasets.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "512ecec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 9427\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3270\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 3245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolq_dataset = ds.load_dataset(\"super_glue\", \"boolq\")\n",
    "boolq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ada14479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 7541\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'passage', 'idx', 'label'],\n",
       "        num_rows: 1886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = boolq_dataset['train']\n",
    "dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")  #stratify_by_column=\"label\"参数确保划分时保持标签列的分布比例一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b46ed",
   "metadata": {},
   "source": [
    "# 数据选取与过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博', '大力推进高校治理能力建设', '坚持事业为上选贤任能'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '在推进“双一流”高校建设进程中，我们要紧紧围绕为党育人、为国育才，找准问题、破解难题，以一流意识和担当精神，大力推进高校的治理能力建设。\\n增强政治引领力。坚持党对高校工作的全面领导，始终把政治建设摆在首位，增强校党委的政治领导力，全面推进党的建设各项工作。落实立德树人根本任务，把培养社会主义建设者和接班人放在中心位置。紧紧抓住思想政治工作这条生命线，全面加强师生思想政治工作，推进“三全育人”综合改革，将思想政治工作贯穿学校教育管理服务全过程，努力让学生成为德才兼备、全面发展的人才。\\n提升人才聚集力。人才是创新的核心要素，创新驱动本质上是人才驱动。要坚持引育并举，建立绿色通道，探索知名专家举荐制，完善“一事一议”支持机制。在大力支持自然科学人才队伍建设的同时，实施哲学社会科学人才工程。立足实际，在条件成熟的学院探索“一院一策”改革。创新科研组织形式，为人才成长创设空间，建设更加崇尚学术、更加追求卓越、更加关爱学生、更加担当有为的学术共同体。\\n培养学生竞争力。遵循学生成长成才的规律培育人才，着力培养具有国际竞争力的拔尖创新人才和各类专门人才，使优势学科、优秀教师、优质资源、优良环境围绕立德树人的根本任务配置。淘汰“水课”，打造“金课”，全力打造世界一流本科教育。深入推进研究生教育综合改革，加强事关国家重大战略的高精尖急缺人才培养，建设具有国际竞争力的研究生教育。\\n激发科技创新力。在国家急需发展的领域挑大梁，就要更加聚焦科技前沿和国家需求，狠抓平台建设，包括加快牵头“武汉光源”建设步伐，积极参与国家实验室建设，建立校级大型科研仪器设备共享平台。关键核心技术领域“卡脖子”问题，归根结底是基础科学研究薄弱。要加大基础研究的支持力度，推进理论、技术和方法创新，鼓励支持重大原创和颠覆性技术创新，催生一批高水平、原创性研究成果。\\n发展社会服务力。在贡献和服务中体现价值，推动合作共建、多元投入的格局，大力推进政产学研用结合，强化科技成果转移转化及产业化。探索校城融合发展、校地联动发展的新模式，深度融入地方创新发展网络，为地方经济社会发展提供人才支撑，不断拓展和优化社会服务网络。\\n涵育文化软实力。加快体制机制改革，优化学校、学部、学院三级评审机制，充分发挥优秀学者特别是德才兼备的年轻学者在学术治理中的重要作用。牢固树立一流意识、紧紧围绕一流目标、认真执行一流标准，让成就一流事业成为普遍追求和行动自觉。培育具有强大凝聚力的大学文化，营造积极团结、向上向善、干事创业的氛围，让大学成为吸引和留住一大批优秀人才建功立业的沃土，让敢干事、肯干事、能干事的人有更多的荣誉感和获得感。\\n建设中国特色、世界一流大学不是等得来、喊得来的，而是脚踏实地拼出来、干出来的。对标一流，深化改革，坚持按章程办学，构建以一流质量标准为核心的制度规范体系，扎实推进学校综合改革，探索更具活力、更富效率的管理体制和运行机制，我们就一定能构建起具有中国特色的现代大学治理体系，进一步提升管理服务水平和工作效能。\\n（作者系武汉大学校长）',\n",
       "  '育才造士，为国之本。党的干部是党和国家事业的中坚力量。习近平总书记深刻指出，“历史和现实都表明，一个政党、一个国家能不能不断培养出优秀领导人才，在很大程度上决定着这个政党、这个国家的兴衰存亡。”新修订的《党政领导干部选拔任用工作条例》，坚持以推进伟大事业为导向，将“事业为上、人岗相适、人事相宜”作为一条重要原则，为做好新时代选人用人工作、建设忠诚干净担当的高素质专业化干部队伍，进一步指明了正确方向。\\n党的干部总是与党的事业紧紧连在一起，伟大事业需要高素质干部，干部要在事业发展中锻炼成长。党的十八大以来，党和国家事业之所以取得历史性成就、发生历史性变革，根本原因是有以习近平同志为核心的党中央坚强领导，有习近平新时代中国特色社会主义思想的科学指引，同时也与广大干部奋发有为、改革创新、干事创业、担当奉献密不可分。当前，中国特色社会主义进入新时代，站在新的历史起点上，我们党要肩负起新的历史使命，必须贯彻新时代党的组织路线，坚持从党和人民事业需要出发选干部、用干部，突出实践实干选贤能，坚持有为有位聚英才，真正做到事业发展需要什么样的人就用什么样的人，什么样的人最合适就选什么样的人。\\n为官择人者治，为人择官者乱。新修订的《干部任用条例》，通篇贯穿事业导向、事业要求，在提出深入考察干部政治素质、道德品行、作风素养、廉政情况的同时，强调要突出工作实绩、履职尽责等方面的考察，大力选拔敢于负责、勇于担当、善于作为、实绩突出的优秀干部。贯彻落实条例，必须正确把握事业发展需要和干部成长进步的关系，知事识人、依事择人、精准选人，把善于统筹推进“五位一体”总体布局和协调推进“四个全面”战略布局，贯彻落实新发展理念、推进高质量发展、深化供给侧结构性改革、打好“三大攻坚战”的优秀干部及时发现出来、合理使用起来，进而示范引领更多干部勇于担当作为，心无旁骛干事业、坚定信心促发展。\\n不拒众流，方为江海。五湖四海的事业，需要五湖四海的人来干。新修订的《干部任用条例》，鲜明提出要注意从企业、高等学校、科研院所等单位以及社会组织中发现选拔党政领导干部，对推动国有企事业单位、社会组织干部人才及时进入党政机关作出制度性安排，这是我们党选人用人成功经验的深刻总结和运用。贯彻落实条例，就要坚持干部工作一盘棋，进一步开阔视野、拓宽渠道，放眼各条战线、各个领域、各个行业、各个层级，充分盘活干部资源，广开进贤之路。要坚持公道正派、公正用人，选拔干部论能力、看水平、凭实绩，而不能搞平衡照顾、论资排辈、降格以求，更不能搞小圈子、任人唯亲，在少数人中选人。要突出政治过硬、本领高强，深入考察识别干部的专业能力、专业素养、专业精神。坚持立足当前、着眼长远，注重发现培养和选拔使用在改革发展稳定一线特别是在重大斗争中经过磨砺的优秀年轻干部，为党和国家事业发展源源不断地注入生机和活力。要坚持严管和厚爱结合、激励和约束并重，认真贯彻习近平总书记关于“三个区分开来”的重要要求，宽容干部在改革创新中的失误错误，保护干部干事创业的积极性。对那些符合有关规定给予容错的干部，要认真落实条例的要求，给予客观公正对待，为他们重整旗鼓、轻装上阵、贡献才智搭建平台。\\n宏伟的事业，离不开高素质专业化的干部。各级党委（党组）及其组织人事部门，要以贯彻落实新修订的《干部任用条例》为契机，坚持党的原则第一、党的事业第一、人民利益第一，大力选拔党和人民需要的好干部，为推动中国特色社会主义伟大事业乘风破浪、不断前进提供坚强组织保证。\\n《 人民日报 》（ 2019年03月20日\\xa0\\xa0 06 版）\\n']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].select(list(range(0,3)))[0:]  #select选取指定的数据建立成一个数据集 这里就选了前3条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a028095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['望海楼美国打“台湾牌”是危险的赌博',\n",
       "  '台湾问题不是美少数反华政客的“翻本筹码”',\n",
       "  '在台湾问题上耍弄心机注定打错算盘',\n",
       "  '人民日报海外版“中国台湾”四个字，踩了谁的痛脚？',\n",
       "  '再说一遍台湾是中国不可分割的一部分！',\n",
       "  '台湾问题决不容任何外来干涉'],\n",
       " 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。对此，中国外交部发言人表示，有关法案严重违反一个中国原则和中美三个联合公报规定，粗暴干涉中国内政，中方对此坚决反对并已向美方提出严正交涉。\\n事实上，中国高度关注美国国内打“台湾牌”、挑战一中原则的危险动向。近年来，作为“亲台”势力大本营的美国国会动作不断，先后通过“与台湾交往法”“亚洲再保证倡议法”等一系列“挺台”法案，“2019财年国防授权法案”也多处触及台湾问题。今年3月，美参院亲台议员再抛“台湾保证法”草案。众院议员继而在4月提出众院版的草案并在近期通过。上述法案的核心目标是强化美台关系，并将台作为美“印太战略”的重要伙伴。同时，“亲台”议员还有意制造事端。今年2月，5名共和党参议员致信众议院议长，促其邀请台湾地区领导人在国会上发表讲话。这一动议显然有悖于美国与台湾的非官方关系，其用心是实质性改变美台关系定位。\\n上述动向出现并非偶然。在中美建交40周年之际，两国关系摩擦加剧，所谓“中国威胁论”再次沉渣泛起。美国对华认知出现严重偏差，对华政策中负面因素上升，保守人士甚至成立了“当前中国威胁委员会”。在此背景下，美国将台海关系作为战略抓手，通过打“台湾牌”在双边关系中增加筹码。特朗普就任后，国会对总统外交政策的约束力和塑造力加强。其实国会推动通过涉台法案对行政部门不具约束力，美政府在2018年并未提升美台官员互访级别，美军舰也没有“访问”台湾港口，保持着某种克制。但从美总统签署国会通过的法案可以看出，国会对外交产生了影响。立法也为政府对台政策提供更大空间。\\n然而，美国需要认真衡量打“台湾牌”成本。首先是美国应对危机的代价。美方官员和学者已明确发出警告，美国卷入台湾问题得不偿失。美国学者曾在媒体发文指出，如果台海爆发危机，美国可能需要“援助”台湾，进而导致新的冷战乃至与中国大陆的冲突。但如果美国让台湾自己面对，则有损美国的信誉，影响美盟友对同盟关系的支持。其次是对中美关系的危害。历史证明，中美合则两利、斗则两伤。中美关系是当今世界最重要的双边关系之一，保持中美关系的稳定发展，不仅符合两国和两国人民的根本利益，也是国际社会的普遍期待。美国蓄意挑战台湾问题的底线，加剧中美关系的复杂性和不确定性，损害两国在重要领域合作，损人又害己。\\n美国打“台湾牌”是一场危险的赌博。台湾问题是中国核心利益，中国政府和人民决不会对此坐视不理。中国敦促美方恪守一个中国原则和中美三个联合公报规定，阻止美国会审议推进有关法案，妥善处理涉台问题。美国悬崖勒马，才是明智之举。\\n（作者系中国国际问题研究院国际战略研究所副所长）',\n",
       "  '新华社北京1月10日电 距离下台不足两周，美国“史上最差国务卿”蓬佩奥突然宣布取消美台官方往来的限制，一语既出，举世皆惊。人们知道蓬氏代表的美国少数反华政客已四面楚歌，会通过“最后疯狂”捞取“翻本筹码”，但他们将“台湾牌”打到如此没有下限，其用心可谓歹毒。\\n四十多年前，中美双方领导人以极大的智慧和创造力在台湾问题上达成一致，这成为两个大国建交的重要基础之一。世界上只有一个中国，台湾是中国领土不可分割的一部分，中华人民共和国政府是代表中国的唯一合法政府，这是国际社会公认的事实，也是美方在中美建交公报等三个联合公报中的明确承诺。\\n毁城容易建城难。美国现今少数政客既无“建城”之才，也无“守城”之愿，他们眼里只有个人政治利益那“一亩三分地”，只会抡起大锤一通乱砸，砸坏中美关系、砸乱国际秩序、砸烂美国国家信誉，通通在所不惜，给继任者留下难以收拾的烂摊子。\\n对外，现任美国政府一手破坏了其前任政府和领导人参与建造的国际体系，将自己塑造成多边合作的敌人；对内，他们煽动对立，最终导致国会山被攻陷，造成平民和警察伤亡，美国社会撕裂前所未有。美国国内、甚至政府内部对这些政客的厌弃也达到了顶峰。\\n所谓病急乱投医，打“中国牌”一直以来就被美一些无良政客视为挽救自己政治生命的“救命稻草”，他们在台湾、香港、新疆等问题上频频插手，干涉中国内政，为的是牵制中国发展，为的是通过扮演反华急先锋，为自己捞取政治资本。在台湾问题上，近来他们的危险行动更是变本加厉。从签署所谓“台湾保证法”，到与台湾方面通过视频形式进行所谓“政治军事对话”，从宣布派美驻联合国代表访台，到宣布取消美台官方往来限制，这些政客在台湾问题上如此不择手段，无非是想转移矛盾，最后捞点“翻本筹码”，根本不考虑中美两国关系与两国人民的长远利益。\\n中美关系是世界上最重要的双边关系。作为世界上第一和第二大经济体，中美关系的好坏，不仅事关两国和两国人民的根本利益，也深刻影响全球福祉和世界局势的走向。\\n台湾问题事关中国主权和领土完整，涉及中国核心利益。中方必将采取一切措施坚决维护自身主权、安全利益。美方如果任由这一小撮人在台湾问题上搞政治操弄，无疑会破坏中美关系的基石。美方一些势力若一意孤行，也势必为此付出沉重代价。（记者 李蓉）',\n",
       "  '新华社纽约9月21日电 美国副国务卿克拉奇9月17日抵达台湾进行活动。美方不顾中方坚决反对和反复规劝，屡屡在台湾问题上耍弄心机，注定打错算盘。任何无视、否定或者挑战一个中国原则的企图都将以失败而告终。\\n台湾问题事关中国主权和领土完整，涉及中国核心利益，中方坚决反对美台以任何借口、任何形式搞官方往来，这一立场是一贯的、明确的。美方执意安排克拉奇访台，背后究竟打的什么小算盘，美方心知肚明，世人也看得明明白白。无非是搞“政治秀”牟取私利，打“台湾牌”牵制中国。\\n一段时间以来，美方在台湾问题上屡屡蓄意挑衅。国务卿蓬佩奥公开祝贺蔡英文就职于前，卫生与公众服务部长阿扎访台于后；出台所谓“台北法案”，妄言支持台湾巩固“邦交”，还向台出售大批军火赚黑心钱。美方的种种错误行径，严重违反国际法和国际关系基本原则，严重违反美方向中方作出的政治承诺，给台海和平稳定带来严重负面影响。世人从中尽可看清美国政客为一己私利而背信弃义、逆时而动的行为，美国原已严重透支的国际信誉也将因此变得更加赤字累累。\\n台湾问题是中美关系中最重要最敏感的问题，一个中国原则是中美关系的政治基础。美方应恪守一个中国原则和中美三个联合公报规定，停止一切形式的美台官方往来，停止向“台独”分裂势力发出任何错误信号，避免严重损害中美关系和台海和平稳定。\\n中国在台湾问题上的立场一以贯之。世界上只有一个中国，台湾是中国不可分割的一部分。中方维护自身主权和安全的决心坚定不移。中国绝不允许任何人、任何组织、任何政党、在任何时候、以任何形式、把任何一块中国领土从中国分裂出去。中国有坚定的意志、充分的信心和足够的能力，挫败任何形式的外部势力干涉和“台独”分裂行径，坚定捍卫国家主权和领土完整，坚定维护台海地区和平稳定。\\n奉劝美国一些利令智昏的政客们，在涉及中方核心利益的问题上，切勿心存幻想和侥幸，切勿低估中方捍卫国家主权和领土完整的坚定决心和意志。美方应充分认清台湾问题的高度敏感性，立即停止美台官方往来，慎重处理涉台问题，不要一错再错、害人误己、玩火自焚。（新华社记者）',\n",
       "  '据媒体报道，台湾通讯传播主管部门近日通过所谓“电信终端设备审验办法”的修改草案，规定手机、平板电脑等电子产品包装、说明书、软件和固件等，不得标示“中国台湾”字样，否则不予审验通过，即使通过的，也撤销证明不得销售。\\n举起政治大刀砍到经济领域，明眼人一看就明白气从何来。只是这气撒得自相矛盾。按照岛内相关法律规定，台湾也是中国的一部分，更不用说“九二共识”和国际社会通行的一中政策了。但民进党当局非要揣着明白装糊涂，比如台湾“流行疫情指挥中心”的所谓“中央”官员，就曾宣布“整个中国已沦为疫区”、“中港澳回台一律自主隔离14天”等论调，即使用语“违法”也不在意，处处显示出政治操作的痕迹。\\n出于政治目的操作政治对立，民进党当局可谓驾轻就熟。说到底，是因为“中国台湾”四个字，踩了“台独”教义的七寸，让有些人不爽了。他们在1月份的选举中意外大胜，开始翘尾巴。在大陆防疫抗疫的紧急关口落井下石，盯着世卫组织妄图寻求“外交突破”，为此大做文章，看到有西方媒体煽风点火，更是一副志得意满的样子。但心里的小九九很清晰也很龌龊，台电子产品所谓新规只是最新的印证而已。\\n这种操作手法很不明智，绝对是损人不利己，因为它破坏了两岸正常的经贸交流。今年以来，大陆品牌手机在台湾一片静悄悄。始作俑者，就是这个电子产品新规。去年底，台湾通讯传播主管部门表示，手机品牌业者必须承诺系统中不会标示“中国台湾”，也不会在系统更新时再调整标示。此举导致今年大陆厂商新手机无法在台销售。大陆手机在台销售，并不是钱都进了陆企的腰包，它能带动岛内就业、拉动岛内消费，作用不容小觑。有舆论认为，为了民进党当局私利而挑战两岸企业最不愿碰触的政治敏感带，绝非台湾经济发展之福。\\n笔者的台湾朋友中，有不少人是华为、小米或者vivo等手机的粉丝。他们长期使用，一步步建立起了对大陆手机性价比高的牢固认知。如今有关部门一纸令下，就剥夺了他们的商品选择权利。这难道不是在伤害台湾民众的切身利益？更何况，操作对立引发大陆网友的强烈反弹。有人说，既然不能标识“中国台湾”，那就叫“中华人民共和国台湾省”如何？否则，岛内相关产品也不要登陆了。虽然是一时气话，但汹汹民意背后，是对政治操作的反感和反弹，不可不察。\\n有业者担心，台电子产品所谓新规“有向大陆宣战的意味”。其实远不仅此，民进党当局一步步操作对立，桩桩件件可不少，几乎要把两岸的政治默契和模糊空间整没了，把两岸的互信和善意整没了。如今台海形势愈发复杂严峻，有识之士忧心忡忡，两岸正在从“冷对立”逐渐走向可能发生“火车头对撞”的险境。\\n虽然相关草案还需要经过公告、收集业界意见后，再送“行政院”、“立法院”审查，但在绿营占据优势的情况下，外界预期修正案将很快获得通过。岛内业者认为，此举等于变相封杀了大陆手机在台湾的销售。岛内经济本来就闷，让政治对抗火烧到经济，将为台湾经济带来浩劫，对谁都没有好处。问题是，不要自废武功的箴言，有人听得进去吗？\\n履深临险，智者不为也。砍斫两岸和平交流的利基，放任敌意螺旋上升，绝非两岸人民之福。这种做法，注定不得人心。',\n",
       "  '新华社北京11月14日电 美国国务卿蓬佩奥简直就是国际政坛的一个异数，最大特点是敢于信口雌黄，尤其在涉及中国的问题上，几乎没说过正确的话。所以，当他日前喊出“台湾不是中国的一部分”时，整个国际社会都将其视为此人又一次疯狂搞笑行为而不予理睬。只有民进党当局欣喜异常，感激涕零。他们在狼狈为奸的时候全都忘了低头看看，自己手里究竟有几张还算有点分量的牌。\\n蓬佩奥讲话常常令人忘了他的身份和头衔。身为美国对外事务的高级决策者之一，他对现当代国际关系史和国际关系基本准则的无知令人叹为观止。世界上只有一个中国，台湾是中国领土不可分割的一部分，中华人民共和国政府是代表中国的唯一合法政府。这既是不容置疑的客观事实，也是国际关系基本准则和国际社会普遍共识。一个中国原则是中美关系的政治基础。1979年中美建交公报明确指出，美国承认中华人民共和国政府是中国的唯一合法政府。40余年来，中美两国几代领导人都是在此基础上用心培育两国人民之间的友好关系，中美双方都从中获益。身为美国的国务卿，蓬佩奥却好像对美国的国家利益何在既不了解，也不在意。他在涉及中美关系上的每一句话都让人觉得，他手里有的不过是一把烂牌。\\n台湾问题事关中国主权和领土完整，事关中国核心利益和中国人民民族感情，不容任何外来干涉，也不容任何不负责任地信口开河。蓬佩奥一而再、再而三在台湾问题上发表极其错误的言论，否认台湾是中国一部分的言谈更是触及中国人民的底线。蓬佩奥实在有必要约束一下自己的嘴，逞一时口舌之快对台湾的民进党其实没有多大帮助，付出的代价却要台湾人民来买单。如果不想在不远的将来像小丑一样下台，还是老老实实地按中美三个联合公报的规定行事吧。\\n一些美国政客出于自身利益玩弄“以台制华”，台湾的民进党当局却误以为自己身价倍增，有了“叫牌”的实力。他们积极主动配合美国遏制中国战略，甘当马前卒。他们面对大陆时口口声声讨要“对等、尊严”，面对美国时却低声下气、卑躬屈膝。他们只为谋一党一己之私，毫不在意中华民族根本利益和台湾同胞切身福祉。他们倚美抗陆、倚美谋“独”，一步步将台湾推向危险境地。如此多行不义，不知将来还有何脸面面对列祖列宗。\\n台湾是中国一部分，这是任何人、任何势力都无法改变的。国家统一是中华民族伟大复兴的历史必然，这是任何人、任何势力都无法阻挡的。中国的地图，一点都不能少；中国的领土，一寸都不能丢。如果有谁还对此有所怀疑而不惜铤而走险，中国人民一定会让他看到我们的决心和能力。',\n",
       "  '原标题：台湾问题决不容任何外来干涉——评美国会通过相关涉台议案\\n新华社北京5月8日电\\u3000美国联邦众议院7日通过所谓“2019年台湾保证法”与“重新确认美国对台及对执行台湾关系法承诺”决议案，再次赤裸裸暴露美国利用台湾问题牵制、遏制中国的阴暗图谋。美方这一危险的政治操弄性质极其恶劣，将对台海和平稳定和中美关系造成严重危害。\\n世界上只有一个中国，大陆和台湾同属一个中国。中华人民共和国是中国的唯一合法政府，中国的主权和领土完整不容分割。坚持一个中国原则是公认的国际关系准则和国际社会的普遍共识，也是发展中美关系的政治基础。近一段时间以来，美方频打“台湾牌”，从对台军售、美台军事交流，到此次炮制恶法要求将台湾纳入双多边军演、推动对台军售常态化、支持台湾加入国际组织等，一再严重违反一个中国原则和中美三个联合公报规定，违背了基本的国际法准则和国际义务。\\n台湾问题事关中国核心利益和中国人民民族感情，是毫无退让、妥协或交易的空间的。在当前两岸关系高度复杂严峻的形势之下，美方不断挑动台海敏感神经，持续向“台独”势力发出错误信号，居心险恶，种种行径坐实了其以台湾为筹码围堵中国的政治盘算。美方操弄台海局势，为求自身利益罔顾两岸中国人利益福祉，不惜将台湾推向危险边缘，充分显露出其彻头彻尾的利己主义和霸权思维。\\n台湾问题是中国人自己的事，理应由中国人自己来决定，不由得外来势力插手干涉。在这个问题上，中国政府和中国人民决不会坐视不理。我们要严正地指出，破坏一个中国原则无异于动摇中美关系的根基，不符合中美两国根本利益。在台湾问题这个中美关系中最重要、最敏感的问题上，美方若不顾中美关系大局，执意在错误道路上走下去，中方必作出有力回应。美方应充分认识到后果。（新华社记者）']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].filter(lambda example:'台湾' in example['title'])[0:]  #过滤 获得标题包含台湾的样本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca204e6e",
   "metadata": {},
   "source": [
    "# 数据映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2626e354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c5a558110b485bb58bb9e61252435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda\\envs\\vit\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fan\\.cache\\huggingface\\hub\\models--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11291a6ef8c5417cb6a5fdd78c59d527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8032840034fb47239b800ff2a83dc2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cd5c3bd760407b8a69befd7eee0e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a9490ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    x = tokenizer(examples[\"content\"], max_length=512, truncation=True)\n",
    "    label = tokenizer(examples[\"title\"], max_length=512, truncation=True)\n",
    "    x['labels'] = label['input_ids']\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945fd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_datasets = datasets.map(preprocess_function, num_proc=4)  # num_proc=4表示使用4个进程进行数据处理\n",
    "process_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9eea167a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d33cb5940f64ab9838bba76d47496cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2126469f952f404eb4160e911e9b115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#批量处理 ,并删去文本内容只保留编码后内容\n",
    "process_datasets = datasets.map(preprocess_function,batched=True, remove_columns=datasets['train'].column_names) \n",
    "process_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35373a",
   "metadata": {},
   "source": [
    "# 保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508c989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fe1fa9287245f397a2fb87964028bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb44d7eac69e4cc09d739c99021452cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 保存处理后的数据集\n",
    "process_datasets.save_to_disk('./processed_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33f8ee2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5850\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "process_datasets = ds.load_from_disk('./processed_datasets')\n",
    "process_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb2334",
   "metadata": {},
   "source": [
    "# 加载本地数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b061586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.load_dataset(\"csv\", data_files = \"../dataset/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset\n",
    "\n",
    "#  dataset = ds.load_dataset(\"csv\", data_dir = \"folder_name\", split=\"train\")  加载文件夹中的所有csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b42da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7766\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.Dataset.from_csv(\"../dataset/ChnSentiCorp_htl_all.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b93fed",
   "metadata": {},
   "source": [
    "# 通过自定义加载脚本加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794339b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found load_cmrc2018.py",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./load_cmrc2018.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\vit\\lib\\site-packages\\datasets\\load.py:1488\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   1484\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1485\u001b[0m )\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1488\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1489\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   1490\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   1491\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   1492\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   1493\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1494\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   1495\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1496\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1497\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1498\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1499\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1501\u001b[0m )\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\vit\\lib\\site-packages\\datasets\\load.py:1133\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[1;32m-> 1133\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\vit\\lib\\site-packages\\datasets\\load.py:918\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(combined_path):\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found load_cmrc2018.py"
     ]
    }
   ],
   "source": [
    "# datasets 新版本不支持scripts 方式加载数据集 在load_cmrc2018.py中尝试了用from_list方法导入了数据集\n",
    "dataset = ds.load_dataset(\"./load_cmrc2018.py\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b9f41",
   "metadata": {},
   "source": [
    "# Datacollator with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eaff8170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaba6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.load_dataset(\"csv\", data_files= \"../dataset/ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)  #滤除脏数据\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e02b74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples['review'], max_length=128, truncation=True)\n",
    "    tokenized_examples['labels'] = examples['label']\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d42b5adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c839a8314445659ee9bbbd538dc9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(process_function, batched = True, remove_columns=dataset.column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e404cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer = tokenizer)  #自动处理批次数据的填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ce2231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625547d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(tokenized_dataset, batch_size=4, collate_fn=collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ec18588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[ 101, 3302, 1218, 2578, 2428, 2141, 1762, 3221, 1922, 2345,  749,  117,\n",
       "          2215, 1071, 3221, 3517, 2231, 4638, 3302, 1218, 1447,  117, 1059, 3221,\n",
       "          1920, 2050,  117, 2578, 2428, 7478, 2382, 4638, 2626, 1219,  106, 3819,\n",
       "          2797, 7313,  671, 3819, 4074, 1059, 3221, 3717,  117, 3819, 5567, 4658,\n",
       "          4638, 3837, 3717, 6862, 2428, 4696, 4638, 3221, 2714, 4638, 5440, 7741,\n",
       "           782, 4638, 5447, 2552,  119, 6821,  702, 6983, 2421, 3146,  860, 5445,\n",
       "          2347, 3683, 6772, 2345,  119, 3302, 1218, 1469, 6392, 3177, 6963,  679,\n",
       "           966, 6929,  702,  817, 7178,  119, 3241,  677, 4125, 6756, 2523, 1427,\n",
       "           119, 2161, 7667, 1353, 7668, 8213, 2399, 8110, 3299, 8124, 3189, 8038,\n",
       "          7478, 2382, 2697, 6468, 2644, 2190, 2769,  812, 4638, 4684, 6241, 2821,\n",
       "          6397, 8024, 6983, 2421, 3175, 2190, 2644,  102],\n",
       "         [ 101, 3315,  782, 3221, 1762, 3217, 5688, 1400,  123, 3299, 8149, 1384,\n",
       "          1057,  857, 4638, 8024, 6421, 6983, 2421, 2523, 2345, 8039, 6432, 3221,\n",
       "          1724, 3215, 5277, 8024, 1377, 3221, 3302, 1218, 2130, 1059, 3766, 3300,\n",
       "          1168,  855,  511, 2769, 1066,  857,  749,  758, 1921, 8039,  676, 1921,\n",
       "          3302, 1218, 1447, 6822, 2791, 7313, 2218, 3221, 1363, 6158, 2094, 1469,\n",
       "          6133,  671, 3613, 2595, 6983, 2421, 4500, 1501, 8039, 1765,  677, 6963,\n",
       "          3766, 3300, 3926, 3815, 6814, 8039, 2769, 5018,  671, 1921, 2957, 4638,\n",
       "          2207, 5291, 2244, 1168, 5018,  676, 1921, 6825, 6720, 4638,  855, 5390,\n",
       "          6963, 3766, 2940, 6814, 8039, 4696, 1377, 5010, 8013, 3241,  677, 2523,\n",
       "          1427, 8024, 2458, 7305, 1898, 7509, 2523, 1510, 8039, 2141, 1762, 2523,\n",
       "          2345, 8039, 2456, 6379, 1920, 2157, 6963,  102],\n",
       "         [ 101, 2791, 7313, 6392, 3177, 8024, 2357, 5390, 6963, 1166, 5636, 8024,\n",
       "          2523, 3300, 4294, 5682, 8024,  671, 6822, 2238, 2218,  679, 2682, 1139,\n",
       "          3341, 1568, 8024,  679, 6814, 2218, 1008, 5381, 1351, 6432, 4638, 1728,\n",
       "           711, 3221, 3312, 3517, 7392, 7509, 2523, 2345, 8024, 3517,  677,  671,\n",
       "          1220, 8024, 3517,  678, 2218, 1420,  677, 1343, 2523, 1920, 1898, 8024,\n",
       "          1962, 1762, 2769,  812, 3221,  697, 2190, 2207, 1923, 1988,  671, 6629,\n",
       "          1343, 4638, 8024, 2218,  857, 1398,  671, 3406,  749, 8024, 4381, 1825,\n",
       "          3315, 6963, 1762, 3517,  677, 8024, 6206, 4717,  749,  671, 2190, 1086,\n",
       "           678, 1343, 4717, 8024, 1728,  711, 3517,  677, 7599, 3250, 3683, 6772,\n",
       "          1962,  511, 5445,  684, 2769, 6225, 2175, 6814,  749, 8024, 4802, 2141,\n",
       "          3221, 5381, 1351, 6382, 4638, 8352, 8835,  102],\n",
       "         [ 101, 6983, 2421, 3302, 1218, 6772, 1962,  511, 1057,  857, 1400, 6820,\n",
       "          5314, 1048, 6589, 1285,  749, 5277, 8024, 4522,  678, 2523, 1962, 4638,\n",
       "          1313, 6496,  511, 3300, 3322,  833, 6820,  833, 1343,  857, 8024,  852,\n",
       "           817, 3419, 2418, 6421, 1086, 7360,  678, 3341,  671,  763,  511,  769,\n",
       "          6858,  679, 3221, 2523, 3175,  912,  511,  102,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0, 0, 1, 1])})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(dl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
