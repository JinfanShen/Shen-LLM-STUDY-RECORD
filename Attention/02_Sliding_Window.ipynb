{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sliding Window Attention (滑动窗口注意力)\n",
                "\n",
                "## 1. Introduction (简介)\n",
                "\n",
                "Used in models like **Mistral 7B**, Sliding Window Attention (SWA) limits each token to attend only to a fixed number of previous tokens ($W$).\n",
                "以 **Mistral 7B** 为代表的模型使用了滑动窗口注意力 (SWA)，它限制每个 token 只能关注过去固定数量的 tokens ($W$)。\n",
                "\n",
                "### Why? (为什么？)\n",
                "1. **Efficiency (效率)**: Complexity reduces from $O(L^2)$ to $O(L \\times W)$. 注意力计算复杂度降低。\n",
                "2. **KV Cache**: You only need to store the last $W$ tokens in the cache (Fixed size Ring Buffer). 只需要固定大小的缓存。\n",
                "\n",
                "### The Trick: Receptive Field (感受野)\n",
                "Even with a window $W$, efficient information propagation happens across layers.\n",
                "- Layer 1: Token $t$ sees $[t-W, t]$.\n",
                "- Layer 2: Token $t$ attends to tokens in Layer 1, which effectively saw $[t-2W, t]$.\n",
                "- ...\n",
                "Thus, deeper layers have a theoretical receptive field much larger than $W$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "def plot_attention_mask(mask, title=\"Attention Mask\"):\n",
                "    plt.figure(figsize=(6, 6))\n",
                "    sns.heatmap(mask.cpu().numpy(), square=True, cbar=False, cmap=\"Blues\")\n",
                "    plt.title(title)\n",
                "    plt.xlabel(\"Key Position\")\n",
                "    plt.ylabel(\"Query Position\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Creating the Mask (创建掩码)\n",
                "\n",
                "Standard Causal Mask allows seeing everything before $t$ (lower triangular).\n",
                "Sliding Window Mask creates a \"band\" (对角带状) matrix.\n",
                "\n",
                "Let's implement a function to generate this mask."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 600x600 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def create_sliding_window_mask(seq_len, window_size):\n",
                "    \"\"\"\n",
                "    Creates a mask where matrix[i, j] = 1 if j <= i and i - j < window_size\n",
                "    Otherwise 0 (or -inf in practice).\n",
                "    \"\"\"\n",
                "    # 1. Create indices\n",
                "    indices = torch.arange(seq_len)\n",
                "    # shape: (seq_len, 1) and (1, seq_len)\n",
                "    rows = indices.view(-1, 1)\n",
                "    cols = indices.view(1, -1)\n",
                "    \n",
                "    # 2. Causal constraint: query (row) must be >= key (col) -> rows >= cols\n",
                "    causal_mask = rows >= cols   #下三角\n",
                "\n",
                "    # 3. Window constraint: distance (rows - cols) must be < window_size\n",
                "    window_mask = (rows - cols) < window_size  #在windows_size之上的上三角\n",
                "    # Combine\n",
                "    final_mask = causal_mask & window_mask\n",
                "    \n",
                "    # Convert to float mask for attention (0 for keep, -inf for mask)\n",
                "    # Usually we add this to attention scores.\n",
                "    # Here we just return 1s and 0s for visualization.\n",
                "    return final_mask.float()\n",
                "\n",
                "# Visualize\n",
                "L = 10\n",
                "W = 5\n",
                "mask = create_sliding_window_mask(L, W)\n",
                "plot_attention_mask(mask, title=f\"Sliding Window Attention (W={W})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Applying it (应用)\n",
                "\n",
                "In a real implementation, you would add this mask to the attention scores before Softmax.\n",
                "\n",
                "```python\n",
                "scores = Q @ K.T / sqrt(d)\n",
                "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
                "probs = softmax(scores)\n",
                "```\n",
                "\n",
                "### Note on \"Rolling Buffer\" Cache\n",
                "Mistral implementation is clever. Instead of shifting tensors (slow), they use a **Rolling Buffer** (rotating index) for the KV cache.\n",
                "\n",
                "Example for Cache Size 4:\n",
                "- Step 1: `[t1, 0, 0, 0]`\n",
                "- Step 4: `[t1, t2, t3, t4]`\n",
                "- Step 5: `[t5, t2, t3, t4]` (Overwrite index 0)\n",
                "- Step 6: `[t5, t6, t3, t4]` (Overwrite index 1)\n",
                "\n",
                "This avoids memory allocation overhead."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.实现Sliding Window Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([1, 8, 128, 128]) tensor([ 2.0678, -3.6940, -0.6472, -7.1638, -1.2749, -3.4011, -2.5821, -0.3916,\n",
                        "         2.4040, -0.3029,  1.6366, -0.0361, -2.5846, -7.1261, -0.3947, -0.5864,\n",
                        "        -4.5202, -1.0975, -3.2847,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
                        "       grad_fn=<SelectBackward0>) tensor([   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  1.7816,\n",
                        "         2.3089,  0.9730, -1.1633,  1.5304, -3.2517,  1.1301,  0.6546,  1.2867,\n",
                        "        -0.3175,  1.3522, -2.5325,  0.5394,  2.0455, -0.0769, -4.5259,  6.2095,\n",
                        "        -0.3752, -2.4702,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
                        "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
                        "       grad_fn=<SelectBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "class SlidingWindowAttention(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, window_size):\n",
                "        \"\"\"\n",
                "        滑动窗口注意力机制\n",
                "        Args:\n",
                "            d_model: 词嵌入维度 d\n",
                "            n_heads: 注意力头的数量 h\n",
                "            window_size: 滑动窗口大小 w\n",
                "        \"\"\"\n",
                "        super(SlidingWindowAttention, self).__init__()\n",
                "        assert d_model % n_heads == 0 ,\"d_model must be divisible by n_heads\"\n",
                "        assert window_size % 2   == 1  ,\"window_size must be odd\"\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.window_size = window_size\n",
                "        self.head_dim = d_model // n_heads  # 每个头的维度\n",
                "        \n",
                "        # 线性投影层\n",
                "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "\n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        前向传播\n",
                "        Args:\n",
                "            x: 输入张量 [batch, seq_len, d_model]\n",
                "        \n",
                "        Returns:\n",
                "            输出张量 [batch, seq_len, d_model]\n",
                "        \"\"\"\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "\n",
                "        # 计算 Q, K, V\n",
                "        Q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # [b, h, seq, d]\n",
                "        K = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "        V = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
                "\n",
                "        # 初始化注意力矩阵\n",
                "        attn_scores = torch.full((batch_size, self.n_heads, seq_len, seq_len), float(\"-inf\"), device=x.device)\n",
                "\n",
                "        # 计算滑动窗口注意力\n",
                "        for i in range(seq_len):\n",
                "            # Symmetric Window: [i - window_size, i + window_size]\n",
                "            start = max(0, i - self.window_size)\n",
                "            end = min(seq_len, i + self.window_size + 1)\n",
                "            \"\"\"\n",
                "            这里的start和end是非因果的，会整合Qi前后的信息\n",
                "            如果是casual(因果)的:\n",
                "            start = max(0, i - self.window_size)\n",
                "            end = min(seq_len, i + 1) \n",
                "            即Qi只能看到Qi当前即之前的K\n",
                "            \"\"\"\n",
                "            # Slice Q to keep dimensions [b, h, 1, d] instead of [b, h, d]\n",
                "            # This fixes the broadcasting error\n",
                "            Q_i = Q[:, :, i:i+1, :] \n",
                "            \n",
                "            # Matmul: [b, h, 1, d] @ [b, h, d, window_len] -> [b, h, 1, window_len]\n",
                "            # Squeeze(2) 为了和左边的维度匹配 [b, h, window_len]\n",
                "            attn_scores[:, :, i, start:end] = torch.matmul(Q_i, K[:, :, start:end, :].transpose(-2, -1)).squeeze(2) \n",
                "\n",
                "        print(attn_scores.shape,attn_scores[0][0][9], attn_scores[0][0][64])\n",
                "        # 归一化\n",
                "        attn_scores /= math.sqrt(self.head_dim)\n",
                "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
                "\n",
                "        # 计算注意力加权的 Value\n",
                "        output = torch.matmul(attn_weights, V)\n",
                "\n",
                "        # 重新排列形状\n",
                "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
                "        output = self.o_proj(output)  # 线性变换回原始维度\n",
                "\n",
                "        return output\n",
                "\n",
                "\n",
                "d_model = 512\n",
                "n_heads = 8\n",
                "window_size = 9\n",
                "\n",
                "swa = SlidingWindowAttention(d_model=d_model, n_heads=n_heads, window_size=window_size)\n",
                "x = torch.randn(1, 128, d_model)\n",
                "y = swa(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([10, 5])\n"
                    ]
                }
            ],
            "source": [
                "#  b的shape会变成[10, 5] 而不是[10, 1, 5]\n",
                "a =  torch.randn(10, 3, 5)\n",
                "b = a[:, 1, :]\n",
                "print(b.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SWA的优缺点\n",
                "\n",
                "#### 优点\n",
                "1.  计算复杂度降低：从 $O(N^2)$ 降到 $O(N*W)$\n",
                "2.  可以拓展到长序列\n",
                "3.  使用于文档级任务\n",
                "#### 缺点\n",
                "1.  不能远距离捕捉依赖\n",
                "2.  需要全局注意力补充\n",
                "\n",
                "### 增强远程依赖能力方法\n",
                "### 1. 结合全局注意力（Gobal Attention）\n",
                "  全局注意力指的是我们将某些位置的Token设置为全局Token，即它会与其它所有Token进行注意力计算。而其它Token则会和窗内Token以及全局Token进行交互。\n",
                "e.g. 假如总共有10个Token，Token 0是全局Token，Token 1-9是窗内Token。 那么Token 0 会和Token 1-9进行注意力计算，Token 1-9会和Token 0 以及窗内Token进行注意力计算。\n",
                "### 2. 结合扩张窗口注意力(Dilated Window Attention)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SWA_With_Global(nn.Module):\n",
                "    def __init__(self, d_model, n_heads, window_size, num_global_tokens=1):\n",
                "        \"\"\"\n",
                "        Sliding Window Attention with Global Tokens\n",
                "        Args:\n",
                "            d_model: embedding dimension d\n",
                "            n_heads: number of attention heads h\n",
                "            window_size: sliding window size w\n",
                "            num_global_tokens: number of global tokens (g) at the start of the sequence.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.n_heads = n_heads\n",
                "        self.window_size = window_size\n",
                "        self.num_global_tokens = num_global_tokens\n",
                "        self.head_dim = d_model // n_heads\n",
                "        \n",
                "        assert self.head_dim * self.n_heads == self.d_model, \"d_model must be divisible by n_heads\"\n",
                "\n",
                "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: input sequence of shape (batch_size, seq_len, d_model)\n",
                "        Returns:\n",
                "            output: attention output of shape (batch_size, seq_len, d_model)\n",
                "        \"\"\"\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        Q = self.q_proj(x).contiguous().view(batch_size, seq_len, self.n_heads, -1).permute(0, 2, 1, 3)\n",
                "        K = self.k_proj(x).contiguous().view(batch_size, seq_len, self.n_heads, -1).permute(0, 2, 1, 3)\n",
                "        V = self.v_proj(x).contiguous().view(batch_size, seq_len, self.n_heads, -1).permute(0, 2, 1, 3)\n",
                "\n",
                "        attn_scores = torch.full((batch_size, self.n_heads, seq_len, seq_len), float('-inf'), device=x.device)\n",
                "        \n",
                "        for i in range(seq_len):\n",
                "            if i < self.num_global_tokens:\n",
                "                # 全局Attention\n",
                "                Qi = Q[:, :, i:i+1, :]\n",
                "                attn_scores[:, :, i, :] = torch.matmul(Qi, K.transpose(-2, -1)).squeeze(2)\n",
                "            else:\n",
                "                # 局部Attention： 窗口＋全局Token\n",
                "                global_end = min(seq_len, self.num_global_tokens)\n",
                "\n",
                "                #窗口indices:\n",
                "                win_start = max(0, i - self.window_size)\n",
                "                win_end = min(seq_len, i + self.window_size + 1)\n",
                "                \n",
                "                #全局Keys:\n",
                "                K_Global = K[:, :, :global_end, :]\n",
                "\n",
                "                #处理窗口和全局Tokens关系\n",
                "                #实际窗口起点：\n",
                "                eff_win_start = max(win_start, global_end)\n",
                "                if eff_win_start < win_end:\n",
                "                    K_window = K[:, :, eff_win_start:win_end, :]\n",
                "                    K_select = torch.cat([K_Global, K_window], dim=2)\n",
                "                    cols = list(range(global_end)) + list(range(eff_win_start, win_end)) #选取做注意力Tokens的位置\n",
                "                else:\n",
                "                    K_select = K_Global\n",
                "                    cols = list(range(global_end))\n",
                "\n",
                "                Qi = Q[:, :, i:i+1, :]\n",
                "                attn_scores[:, :, i, cols] = torch.matmul(Qi, K_select.transpose(-2, -1)).squeeze(2)\n",
                "        \n",
                "        attn_scores /= math.sqrt(self.head_dim)\n",
                "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
                "\n",
                "        output = torch.matmul(attn_scores, V)\n",
                "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
                "        output = self.o_proj(output)\n",
                "\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Forward pass successful! Output shape: torch.Size([1, 10, 64])\n"
                    ]
                }
            ],
            "source": [
                "d_model = 64\n",
                "n_heads = 4\n",
                "window_size = 2\n",
                "num_global = 2\n",
                "seq_len = 10\n",
                "\n",
                "model = SWA_With_Global(d_model, n_heads, window_size, num_global_tokens=num_global)\n",
                "x = torch.randn(1, seq_len, d_model, requires_grad=True)\n",
                "    \n",
                "    \n",
                "output = model(x)\n",
                "print(\"Forward pass successful! Output shape:\", output.shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "vit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.24"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
