{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Query Attention (MQA) & Grouped-Query Attention (GQA)\n",
    "\n",
    "## 1. Introduction (简介)\n",
    "\n",
    "在标准的 **Multi-Head Attention (MHA)** 中，每个 Head 都有自己独立的 Query, Key, 和 Value 矩阵。\n",
    "然而，而在推理阶段 (Inference)，KV Cache (Key-Value 缓存) 会占用大量的显存 (VRAM)，并且会成为计算瓶颈 (Memory Bandwidth Bound)。\n",
    "\n",
    "为了解决这个问题，研究人员提出了 **MQA** 和 **GQA**。\n",
    "\n",
    "- **MHA (Multi-Head Attention)**: 标准做法。每个 Query Head 对应一个 Key Head 和一个 Value Head。\n",
    "- **MQA (Multi-Query Attention)**: 极端优化。所有 Query Heads **共享**同一个 Key Head 和 Value Head。\n",
    "- **GQA (Grouped-Query Attention)**: 折中方案 (LLaMA 2/3 采用)。将 Query Heads 分组，每组共享一个 Key/Value Head。\n",
    "\n",
    "### Visual Comparison (视觉对比)\n",
    "\n",
    "假设 Number of Heads ($H$) = 4。\n",
    "\n",
    "**MHA**: 1:1 ratio\n",
    "```\n",
    "Query Heads: [Q1] [Q2] [Q3] [Q4]\n",
    "              |    |    |    |\n",
    "Key Heads:   [K1] [K2] [K3] [K4]\n",
    "Value Heads: [V1] [V2] [V3] [V4]\n",
    "```\n",
    "\n",
    "**MQA**: H:1 ratio\n",
    "```\n",
    "Query Heads: [Q1] [Q2] [Q3] [Q4]\n",
    "              \\    |    |    /\n",
    "               \\   |    |   /\n",
    "Key Head:        [K_shared]\n",
    "Value Head:      [V_shared]\n",
    "```\n",
    "\n",
    "**GQA**: G:1 ratio (e.g., 2 groups)\n",
    "```\n",
    "Query Heads: [Q1] [Q2]   [Q3] [Q4]\n",
    "              \\   /       \\   /\n",
    "Group 1:       [K1]        [K2]\n",
    "               [V1]        [V2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2632bad7e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Setting seed for reproducibility (不仅设置随机种子，确保可复现)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementations (代码实现)\n",
    "\n",
    "### 2.1 Standard Multi-Head Attention (MHA)\n",
    "这是为了对比用的基准 (Baseline)。\n",
    "\n",
    "Note: In standard MHA, `n_kv_heads` is equal to `n_heads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # W_q, W_k, W_v projections\n",
    "        # For MHA, total output dimension for K and V is same as Q: d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Project Q, K, V\n",
    "        # shape: (B, L, H, D_head)\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 2. Scaled Dot-Product Attention\n",
    "        # (B, H, L, D) @ (B, H, D, L) -> (B, H, L, L)\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_probs = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 3. Apply attention to V\n",
    "        # (B, H, L, L) @ (B, H, L, D) -> (B, H, L, D)\n",
    "        context = attn_probs @ v\n",
    "        \n",
    "        # 4. Concatenate heads and output projection\n",
    "        # (B, H, L, D) -> (B, L, H, D) -> (B, L, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grouped-Query Attention (GQA)\n",
    "\n",
    "Here, `n_kv_heads` is smaller than `n_heads`. Usually `n_heads` is a multiple of `n_kv_heads`.\n",
    "- If `n_kv_heads == 1`, it becomes **MQA**.\n",
    "- If `n_kv_heads == n_heads`, it becomes **MHA**.\n",
    "\n",
    "Key Step: **Repeat / Expand** the KV heads to match Q heads for calculation.\n",
    "关键步骤：在计算 Attention 之前，需要将 KV heads 进行**复制/广播 (repeat/broadcast)**，以对齐 Query heads 的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        \n",
    "        self.head_dim = d_model // n_heads  # Head dimension\n",
    "        # Number of Q heads sharing one KV head\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        \n",
    "        # W_q projects to n_heads\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.head_dim)\n",
    "        \n",
    "        # W_k, W_v project to n_kv_heads (Smaller than n_heads!)\n",
    "        self.W_k = nn.Linear(d_model, n_kv_heads * self.head_dim)\n",
    "        self.W_v = nn.Linear(d_model, n_kv_heads * self.head_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 1. Project\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 2. Repeat KV heads to match Q heads\n",
    "        # k shape: (B, n_kv_heads, L, D)\n",
    "        # We want: (B, n_heads, L, D)\n",
    "        # Method: Insert a dimension for groups, repeat, then flatten\n",
    "        def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "            \"\"\"\n",
    "            Expected shape: (batch, n_kv_heads, seqlen, head_dim)\n",
    "            Return shape: (batch, n_heads, seqlen, head_dim)\n",
    "            \"\"\"\n",
    "            batch, num_kv_heads, slen, head_dim = hidden_states.shape\n",
    "            if n_rep == 1:\n",
    "                return hidden_states\n",
    "            \n",
    "            # (B, n_kv_heads, 1, L, D) -> (B, n_kv_heads, n_rep, L, D) -> (B, n_heads, L, D)\n",
    "            hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_kv_heads, n_rep, slen, head_dim)\n",
    "            return hidden_states.reshape(batch, num_kv_heads * n_rep, slen, head_dim)\n",
    "\n",
    "        k = repeat_kv(k, self.n_rep)\n",
    "        v = repeat_kv(v, self.n_rep)\n",
    "        \n",
    "        # 3. Scaled Dot-Product Attention (Same as MHA now)\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_probs = F.softmax(scores, dim=-1)\n",
    "        context = attn_probs @ v\n",
    "        \n",
    "        # 4. Output\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison & Impact (对比与影响)\n",
    "\n",
    "Let's test the parameter count difference.\n",
    "测试一下参数量的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA Params (n_heads=8, n_kv_heads=8): 1,050,624\n",
      "GQA Params (n_heads=8, n_kv_heads=2): 656,640\n",
      "MQA Params (n_heads=8, n_kv_heads=1): 590,976\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "\n",
    "# 1. MHA\n",
    "mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "mha_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"MHA Params (n_heads=8, n_kv_heads=8): {mha_params:,}\")\n",
    "\n",
    "# 2. GQA\n",
    "# Reducing KV heads by factor of 4 (n_kv_heads = 2)\n",
    "gqa = GroupedQueryAttention(d_model=d_model, n_heads=n_heads, n_kv_heads=2)\n",
    "gqa_params = sum(p.numel() for p in gqa.parameters())\n",
    "print(f\"GQA Params (n_heads=8, n_kv_heads=2): {gqa_params:,}\")\n",
    "\n",
    "# 3. MQA\n",
    "# Extreme case: Only 1 KV head\n",
    "mqa = GroupedQueryAttention(d_model=d_model, n_heads=n_heads, n_kv_heads=1)\n",
    "mqa_params = sum(p.numel() for p in mqa.parameters())\n",
    "print(f\"MQA Params (n_heads=8, n_kv_heads=1): {mqa_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this important? (为什么这很重要？)\n",
    "\n",
    "Although the parameter reduction looks small (parameters are just weights), the **KV Cache Memory** saving is huge during inference.\n",
    "虽然参数量减少看起来不多，但在推理时，**KV Cache 显存**的节省是巨大的。\n",
    "\n",
    "KV Cache Size = $2 \\times \\text{batch\\_size} \\times \\text{seq\\_len} \\times \\text{n\\_kv\\_heads} \\times \\text{head\\_dim} \\times \\text{precision}$\n",
    "\n",
    "If we use GQA with group=4 (n_kv_heads = n_heads / 4), we reduce KV Cache memory by **4x**.\n",
    "This allows for:\n",
    "1. **Larger Batch Sizes**: Higher throughput (这就意味着更大的吞吐量)。\n",
    "2. **Longer Context Windows**: Process longer documents (处理更长的文档)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
