# STUDY-RECORD INTRODUCTION
个人的学习大模型的学习笔记和资料保存。

学习内容：
## Attenion
--_MQA_GQA.ipynb:包含基本多头自注意力（MHA）、多查询注意力（MQA）和分组查询自注意力（GQA）。

--_sliding_Window.ipynb:包含滑窗注意力及和全局注意力一起的混合注意力机制。

参考资料：
1. https://blog.csdn.net/shizheng_Li/article/details/145809397
2. Generated by gemini3
## RoPE编码
--RoPE.ipynb:包含RoPE编码的实现。

--RoPE_Explanation.ipynb:解释了为什么实现RoPE编码时是按前后各一半来转换的，而不是论文里一对一对配对旋转来实现的。

参考资料：
1. https://yuanchaofa.com/post/hands-on-rope-position-embedding.
## Transformers库的使用
--model.ipynb:使用transformers库的model方法。

--pipeline.ipynb:使用transformers库的pipeline方法。

--tokenizer.ipynb:使用transformers库的tokenizer方法。

--classifier_demo.ipynb:使用transformers库实现基础的文本分类训练推理过程。

--Dataset.ipynb:使用Datasets库方法。

--load_cmrc2018.py：读取cmrc2018数据集的实现。Datasets库新版本不支持scripts,所以在该文件中尝试from_list导入。

--Evaluate.py:使用evaluate库的方式。

--Trainer.py: 使用transformers.Trainer训练之前的demo
参考资料或使用的数据集：
1. https://www.bilibili.com/video/BV18T411t7h6
2. https://github.com/SophonPlus/ChineseNlpCorpus
3. https://huggingface.co/docs/transformers/main/en/index
4. https://huggingface.co/datasets/madao33/new-title-chinese
5. cmrc2018数据集是从第一个B站UP主github下载 我也没找到原始地点
6. https://huggingface.co/docs/transformers/trainer