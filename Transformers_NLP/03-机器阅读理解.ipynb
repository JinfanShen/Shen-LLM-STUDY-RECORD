{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d708462",
   "metadata": {},
   "source": [
    "# 1.机器阅读理解\n",
    "## 评估指标\n",
    "1 精准匹配度(Exact Match, EM)：计算预测结果与标准答案是否完全匹配\n",
    "2 模糊匹配度(F1)：计算预测结果与标准答案之间字符的匹配度  \n",
    "e.g:  \n",
    "模型预测结果：北京  \n",
    "真实标签结果：北京天安门\n",
    "\n",
    "$$EM = 0; $$\n",
    "$$\\text{Precision} = \\frac{2}{2}, \\quad \\text{Recall} = \\frac{2}{5}, \\quad F1 = \\frac{2 \\times \\frac{2}{2} \\times \\frac{2}{5}}{\\frac{2}{5} + \\frac{2}{2}} = \\frac{4}{7};$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92552f",
   "metadata": {},
   "source": [
    "# 2.基于Transformers的解决方案\n",
    "## 数据预处理\n",
    "- 数据处理格式：\n",
    "  [CLS] Question [SEP] Context [SEP]\n",
    "- 定位答案位置：\n",
    "  start_positions 和 end_positions (这里的position是Token的位置，不是字符位置)\n",
    "  offset_mapping (将Token映射到字符位置)\n",
    "- Context过长解决策略：\n",
    "  1. 直接截断，实现比较简单，但是会损失答案靠后的信息。\n",
    "  2. 滑动窗口，实现较复杂，但是会保留答案靠后的信息。\n",
    "## 模型结构\n",
    "- *ModelForQuestionAnswering\n",
    "```python\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels #这里的num_labels是2 起始位置和终点位置\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels) \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        start_positions: Optional[torch.Tensor] = None,\n",
    "        end_positions: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)  #[batch_size, seq_len, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1) #[batch_size, seq_len, 1]\n",
    "        start_logits = start_logits.squeeze(-1).contiguous() #[batch_size, seq_len]\n",
    "        end_logits = end_logits.squeeze(-1).contiguous() \n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index) #删去[SEP]标签\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f906f",
   "metadata": {},
   "source": [
    "# 3.基于截断策略的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205978b9",
   "metadata": {},
   "source": [
    "## steps1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e846be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import  load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ceedb6",
   "metadata": {},
   "source": [
    "## steps2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a6d95ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('../dataset/cmrc2018')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a0037f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25938a8",
   "metadata": {},
   "source": [
    "## steps3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56a9f481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b669ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'context', 'question', 'answers'],\n",
       "     num_rows: 10\n",
       " }),\n",
       " ['范廷颂是什么时候被任为主教的？',\n",
       "  '1990年，范廷颂担任什么职务？',\n",
       "  '范廷颂是于何时何地出生的？',\n",
       "  '1994年3月，范廷颂担任什么职务？',\n",
       "  '范廷颂是何时去世的？',\n",
       "  '安雅·罗素法参加了什么比赛获得了亚军？',\n",
       "  'Russell Tanoue对安雅·罗素法的评价是什么？',\n",
       "  '安雅·罗素法合作过的香港杂志有哪些？',\n",
       "  '毕业后的安雅·罗素法职业是什么？',\n",
       "  '岬太郎在第一次南葛市生活时的搭档是谁？'],\n",
       " ['范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "  '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "  '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "  '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "  '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       "  '安雅·罗素法（，），来自俄罗斯圣彼得堡的模特儿。她是《全美超级模特儿新秀大赛》第十季的亚军。2008年，安雅宣布改回出生时的名字：安雅·罗素法（Anya Rozova），在此之前是使用安雅·冈（）。安雅于俄罗斯出生，后来被一个居住在美国夏威夷群岛欧胡岛檀香山的家庭领养。安雅十七岁时曾参与香奈儿、路易·威登及芬迪（Fendi）等品牌的非正式时装秀。2007年，她于瓦伊帕胡高级中学毕业。毕业后，她当了一名售货员。她曾为Russell Tanoue拍摄照片，Russell Tanoue称赞她是「有前途的新面孔」。安雅在半准决赛面试时说她对模特儿行业充满热诚，所以参加全美超级模特儿新秀大赛。她于比赛中表现出色，曾五次首名入围，平均入围顺序更拿下历届以来最优异的成绩(2.64)，另外胜出三次小挑战，分别获得与评判尼祖·百克拍照、为柠檬味道的七喜拍摄广告的机会及十万美元、和盖马蒂洛（Gai Mattiolo）设计的晚装。在最后两强中，安雅与另一名参赛者惠妮·汤姆森为范思哲走秀，但评判认为她在台上不够惠妮突出，所以选了惠妮当冠军，安雅屈居亚军(但就整体表现来说，部份网友认为安雅才是第十季名副其实的冠军。)安雅在比赛拿五次第一，也胜出多次小挑战。安雅赛后再次与Russell Tanoue合作，为2008年4月30日出版的MidWeek杂志拍摄封面及内页照。其后她参加了V杂志与Supreme模特儿公司合办的模特儿选拔赛2008。她其后更与Elite签约。最近她与香港的模特儿公司 Style International Management 签约，并在香港发展其模特儿事业。她曾在很多香港的时装杂志中任模特儿，《Jet》、《东方日报》、《Elle》等。',\n",
       "  '安雅·罗素法（，），来自俄罗斯圣彼得堡的模特儿。她是《全美超级模特儿新秀大赛》第十季的亚军。2008年，安雅宣布改回出生时的名字：安雅·罗素法（Anya Rozova），在此之前是使用安雅·冈（）。安雅于俄罗斯出生，后来被一个居住在美国夏威夷群岛欧胡岛檀香山的家庭领养。安雅十七岁时曾参与香奈儿、路易·威登及芬迪（Fendi）等品牌的非正式时装秀。2007年，她于瓦伊帕胡高级中学毕业。毕业后，她当了一名售货员。她曾为Russell Tanoue拍摄照片，Russell Tanoue称赞她是「有前途的新面孔」。安雅在半准决赛面试时说她对模特儿行业充满热诚，所以参加全美超级模特儿新秀大赛。她于比赛中表现出色，曾五次首名入围，平均入围顺序更拿下历届以来最优异的成绩(2.64)，另外胜出三次小挑战，分别获得与评判尼祖·百克拍照、为柠檬味道的七喜拍摄广告的机会及十万美元、和盖马蒂洛（Gai Mattiolo）设计的晚装。在最后两强中，安雅与另一名参赛者惠妮·汤姆森为范思哲走秀，但评判认为她在台上不够惠妮突出，所以选了惠妮当冠军，安雅屈居亚军(但就整体表现来说，部份网友认为安雅才是第十季名副其实的冠军。)安雅在比赛拿五次第一，也胜出多次小挑战。安雅赛后再次与Russell Tanoue合作，为2008年4月30日出版的MidWeek杂志拍摄封面及内页照。其后她参加了V杂志与Supreme模特儿公司合办的模特儿选拔赛2008。她其后更与Elite签约。最近她与香港的模特儿公司 Style International Management 签约，并在香港发展其模特儿事业。她曾在很多香港的时装杂志中任模特儿，《Jet》、《东方日报》、《Elle》等。',\n",
       "  '安雅·罗素法（，），来自俄罗斯圣彼得堡的模特儿。她是《全美超级模特儿新秀大赛》第十季的亚军。2008年，安雅宣布改回出生时的名字：安雅·罗素法（Anya Rozova），在此之前是使用安雅·冈（）。安雅于俄罗斯出生，后来被一个居住在美国夏威夷群岛欧胡岛檀香山的家庭领养。安雅十七岁时曾参与香奈儿、路易·威登及芬迪（Fendi）等品牌的非正式时装秀。2007年，她于瓦伊帕胡高级中学毕业。毕业后，她当了一名售货员。她曾为Russell Tanoue拍摄照片，Russell Tanoue称赞她是「有前途的新面孔」。安雅在半准决赛面试时说她对模特儿行业充满热诚，所以参加全美超级模特儿新秀大赛。她于比赛中表现出色，曾五次首名入围，平均入围顺序更拿下历届以来最优异的成绩(2.64)，另外胜出三次小挑战，分别获得与评判尼祖·百克拍照、为柠檬味道的七喜拍摄广告的机会及十万美元、和盖马蒂洛（Gai Mattiolo）设计的晚装。在最后两强中，安雅与另一名参赛者惠妮·汤姆森为范思哲走秀，但评判认为她在台上不够惠妮突出，所以选了惠妮当冠军，安雅屈居亚军(但就整体表现来说，部份网友认为安雅才是第十季名副其实的冠军。)安雅在比赛拿五次第一，也胜出多次小挑战。安雅赛后再次与Russell Tanoue合作，为2008年4月30日出版的MidWeek杂志拍摄封面及内页照。其后她参加了V杂志与Supreme模特儿公司合办的模特儿选拔赛2008。她其后更与Elite签约。最近她与香港的模特儿公司 Style International Management 签约，并在香港发展其模特儿事业。她曾在很多香港的时装杂志中任模特儿，《Jet》、《东方日报》、《Elle》等。',\n",
       "  '安雅·罗素法（，），来自俄罗斯圣彼得堡的模特儿。她是《全美超级模特儿新秀大赛》第十季的亚军。2008年，安雅宣布改回出生时的名字：安雅·罗素法（Anya Rozova），在此之前是使用安雅·冈（）。安雅于俄罗斯出生，后来被一个居住在美国夏威夷群岛欧胡岛檀香山的家庭领养。安雅十七岁时曾参与香奈儿、路易·威登及芬迪（Fendi）等品牌的非正式时装秀。2007年，她于瓦伊帕胡高级中学毕业。毕业后，她当了一名售货员。她曾为Russell Tanoue拍摄照片，Russell Tanoue称赞她是「有前途的新面孔」。安雅在半准决赛面试时说她对模特儿行业充满热诚，所以参加全美超级模特儿新秀大赛。她于比赛中表现出色，曾五次首名入围，平均入围顺序更拿下历届以来最优异的成绩(2.64)，另外胜出三次小挑战，分别获得与评判尼祖·百克拍照、为柠檬味道的七喜拍摄广告的机会及十万美元、和盖马蒂洛（Gai Mattiolo）设计的晚装。在最后两强中，安雅与另一名参赛者惠妮·汤姆森为范思哲走秀，但评判认为她在台上不够惠妮突出，所以选了惠妮当冠军，安雅屈居亚军(但就整体表现来说，部份网友认为安雅才是第十季名副其实的冠军。)安雅在比赛拿五次第一，也胜出多次小挑战。安雅赛后再次与Russell Tanoue合作，为2008年4月30日出版的MidWeek杂志拍摄封面及内页照。其后她参加了V杂志与Supreme模特儿公司合办的模特儿选拔赛2008。她其后更与Elite签约。最近她与香港的模特儿公司 Style International Management 签约，并在香港发展其模特儿事业。她曾在很多香港的时装杂志中任模特儿，《Jet》、《东方日报》、《Elle》等。',\n",
       "  '为日本漫画足球小将翼的一个角色，自小父母离异，与父亲一起四处为家，每个地方也是待一会便离开，但他仍然能够保持优秀的学业成绩。在第一次南葛市生活时，与同样就读于南葛小学的大空翼为黄金拍档，曾效力球队包括南葛小学、南葛高中、日本少年队、日本青年军、日本奥运队。效力日本青年军期间，因救同母异父的妹妹导致被车撞至断脚，在决赛周只在决赛的下半场十五分钟开始上场，成为日本队夺得世青冠军的其中一名功臣。基本资料绰号：球场上的艺术家出身地：日本南葛市诞生日：5月5日星座：金牛座球衣号码：11担任位置：中场、攻击中场、右中场擅长脚：右脚所属队伍：盘田山叶故事发展岬太郎在小学期间不断转换学校，在南葛小学就读时在全国大赛中夺得冠军；国中三年随父亲孤单地在法国留学；回国后三年的高中生涯一直输给日本王牌射手日向小次郎率领的东邦学院。在【Golden 23】年代，大空翼、日向小次郎等名将均转战海外，他与松山光、三杉淳组成了「3M」组合（松山光Hikaru Matsuyama、岬太郎Taro Misaki、三杉淳Jyun Misugi）。必杀技1. 回力刀射门2. S. S. S. 射门3. 双人射门(与大空翼合作)'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = dataset['train'].select(range(10))\n",
    "# datasets.Dataset的列返回的是datasets.Column对象 这里需要转换成list格式\n",
    "questions = list(sample_dataset[\"question\"])\n",
    "contexts = list(sample_dataset[\"context\"])\n",
    "# answers = list(sample_dataset['answers'])\n",
    "sample_dataset, questions, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f2ad0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample = tokenizer(text=questions,\n",
    "                            text_pair=contexts,\n",
    "                            return_offsets_mapping=True,\n",
    "                            max_length=384,\n",
    "                            truncation=\"only_second\",\n",
    "                            padding=\"max_length\")\n",
    "tokenized_sample.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4686583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (3, 4),\n",
       " (4, 5),\n",
       " (5, 6),\n",
       " (6, 7),\n",
       " (7, 8),\n",
       " (8, 9),\n",
       " (9, 10),\n",
       " (10, 11),\n",
       " (11, 12),\n",
       " (12, 13),\n",
       " (13, 14),\n",
       " (14, 15),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (1, 2),\n",
       " (2, 3),\n",
       " (3, 4),\n",
       " (4, 5),\n",
       " (5, 6),\n",
       " (6, 7),\n",
       " (7, 8),\n",
       " (8, 9),\n",
       " (9, 10),\n",
       " (10, 11),\n",
       " (11, 12),\n",
       " (12, 13),\n",
       " (13, 14),\n",
       " (14, 15),\n",
       " (15, 16),\n",
       " (16, 17),\n",
       " (17, 18),\n",
       " (18, 19),\n",
       " (19, 20),\n",
       " (20, 21),\n",
       " (21, 22),\n",
       " (22, 23),\n",
       " (23, 24),\n",
       " (24, 25),\n",
       " (25, 26),\n",
       " (26, 27),\n",
       " (27, 28),\n",
       " (28, 29),\n",
       " (29, 30),\n",
       " (30, 34),\n",
       " (34, 35),\n",
       " (35, 36),\n",
       " (36, 37),\n",
       " (37, 38),\n",
       " (38, 39),\n",
       " (39, 40),\n",
       " (40, 41),\n",
       " (41, 45),\n",
       " (45, 46),\n",
       " (46, 47),\n",
       " (47, 48),\n",
       " (48, 49),\n",
       " (49, 50),\n",
       " (50, 51),\n",
       " (51, 52),\n",
       " (52, 53),\n",
       " (53, 54),\n",
       " (54, 55),\n",
       " (55, 56),\n",
       " (56, 57),\n",
       " (57, 58),\n",
       " (58, 59),\n",
       " (59, 60),\n",
       " (60, 61),\n",
       " (61, 62),\n",
       " (62, 63),\n",
       " (63, 67),\n",
       " (67, 68),\n",
       " (68, 69),\n",
       " (69, 70),\n",
       " (70, 71),\n",
       " (71, 72),\n",
       " (72, 73),\n",
       " (73, 74),\n",
       " (74, 75),\n",
       " (75, 76),\n",
       " (76, 77),\n",
       " (77, 78),\n",
       " (78, 79),\n",
       " (79, 80),\n",
       " (80, 81),\n",
       " (81, 82),\n",
       " (82, 83),\n",
       " (83, 84),\n",
       " (84, 85),\n",
       " (85, 86),\n",
       " (86, 87),\n",
       " (87, 91),\n",
       " (91, 92),\n",
       " (92, 93),\n",
       " (93, 94),\n",
       " (94, 95),\n",
       " (95, 96),\n",
       " (96, 97),\n",
       " (97, 98),\n",
       " (98, 99),\n",
       " (99, 100),\n",
       " (100, 101),\n",
       " (101, 105),\n",
       " (105, 106),\n",
       " (106, 107),\n",
       " (107, 108),\n",
       " (108, 110),\n",
       " (110, 111),\n",
       " (111, 112),\n",
       " (112, 113),\n",
       " (113, 114),\n",
       " (114, 115),\n",
       " (115, 116),\n",
       " (116, 117),\n",
       " (117, 118),\n",
       " (118, 119),\n",
       " (119, 120),\n",
       " (120, 121),\n",
       " (121, 122),\n",
       " (122, 123),\n",
       " (123, 124),\n",
       " (124, 125),\n",
       " (125, 126),\n",
       " (126, 127),\n",
       " (127, 128),\n",
       " (128, 129),\n",
       " (129, 130),\n",
       " (130, 131),\n",
       " (131, 132),\n",
       " (132, 133),\n",
       " (133, 134),\n",
       " (134, 135),\n",
       " (135, 136),\n",
       " (136, 137),\n",
       " (137, 138),\n",
       " (138, 139),\n",
       " (139, 140),\n",
       " (140, 141),\n",
       " (141, 142),\n",
       " (142, 143),\n",
       " (143, 144),\n",
       " (144, 145),\n",
       " (145, 146),\n",
       " (146, 147),\n",
       " (147, 148),\n",
       " (148, 149),\n",
       " (149, 150),\n",
       " (150, 151),\n",
       " (151, 152),\n",
       " (152, 153),\n",
       " (153, 154),\n",
       " (154, 155),\n",
       " (155, 156),\n",
       " (156, 157),\n",
       " (157, 158),\n",
       " (158, 159),\n",
       " (159, 163),\n",
       " (163, 164),\n",
       " (164, 165),\n",
       " (165, 166),\n",
       " (166, 167),\n",
       " (167, 168),\n",
       " (168, 169),\n",
       " (169, 170),\n",
       " (170, 171),\n",
       " (171, 172),\n",
       " (172, 173),\n",
       " (173, 174),\n",
       " (174, 175),\n",
       " (175, 176),\n",
       " (176, 177),\n",
       " (177, 178),\n",
       " (178, 179),\n",
       " (179, 180),\n",
       " (180, 181),\n",
       " (181, 182),\n",
       " (182, 186),\n",
       " (186, 187),\n",
       " (187, 188),\n",
       " (188, 189),\n",
       " (189, 190),\n",
       " (190, 191),\n",
       " (191, 192),\n",
       " (192, 193),\n",
       " (193, 194),\n",
       " (194, 195),\n",
       " (195, 196),\n",
       " (196, 197),\n",
       " (197, 198),\n",
       " (198, 199),\n",
       " (199, 200),\n",
       " (200, 201),\n",
       " (201, 202),\n",
       " (202, 203),\n",
       " (203, 204),\n",
       " (204, 205),\n",
       " (205, 206),\n",
       " (206, 207),\n",
       " (207, 208),\n",
       " (208, 209),\n",
       " (209, 210),\n",
       " (210, 211),\n",
       " (211, 212),\n",
       " (212, 213),\n",
       " (213, 214),\n",
       " (214, 215),\n",
       " (215, 216),\n",
       " (216, 217),\n",
       " (217, 218),\n",
       " (218, 222),\n",
       " (222, 223),\n",
       " (223, 224),\n",
       " (224, 225),\n",
       " (225, 226),\n",
       " (226, 227),\n",
       " (227, 228),\n",
       " (228, 229),\n",
       " (229, 230),\n",
       " (230, 231),\n",
       " (231, 232),\n",
       " (232, 233),\n",
       " (233, 234),\n",
       " (234, 235),\n",
       " (235, 236),\n",
       " (236, 237),\n",
       " (237, 238),\n",
       " (238, 239),\n",
       " (239, 240),\n",
       " (240, 241),\n",
       " (241, 242),\n",
       " (242, 243),\n",
       " (243, 244),\n",
       " (244, 245),\n",
       " (245, 246),\n",
       " (246, 247),\n",
       " (247, 248),\n",
       " (248, 249),\n",
       " (249, 250),\n",
       " (250, 251),\n",
       " (251, 252),\n",
       " (252, 253),\n",
       " (253, 257),\n",
       " (257, 258),\n",
       " (258, 259),\n",
       " (259, 260),\n",
       " (260, 261),\n",
       " (261, 262),\n",
       " (262, 263),\n",
       " (263, 264),\n",
       " (264, 265),\n",
       " (265, 266),\n",
       " (266, 267),\n",
       " (267, 268),\n",
       " (268, 269),\n",
       " (269, 270),\n",
       " (270, 271),\n",
       " (271, 272),\n",
       " (272, 273),\n",
       " (273, 274),\n",
       " (274, 275),\n",
       " (275, 276),\n",
       " (276, 277),\n",
       " (277, 278),\n",
       " (278, 279),\n",
       " (279, 280),\n",
       " (280, 281),\n",
       " (281, 282),\n",
       " (282, 283),\n",
       " (283, 284),\n",
       " (284, 285),\n",
       " (285, 286),\n",
       " (286, 287),\n",
       " (287, 288),\n",
       " (288, 289),\n",
       " (289, 290),\n",
       " (290, 291),\n",
       " (291, 292),\n",
       " (292, 293),\n",
       " (293, 294),\n",
       " (294, 295),\n",
       " (295, 296),\n",
       " (296, 297),\n",
       " (297, 298),\n",
       " (298, 299),\n",
       " (299, 300),\n",
       " (300, 301),\n",
       " (301, 302),\n",
       " (302, 303),\n",
       " (303, 304),\n",
       " (304, 305),\n",
       " (305, 306),\n",
       " (306, 307),\n",
       " (307, 308),\n",
       " (308, 309),\n",
       " (309, 310),\n",
       " (310, 311),\n",
       " (311, 312),\n",
       " (312, 313),\n",
       " (313, 314),\n",
       " (314, 315),\n",
       " (315, 316),\n",
       " (316, 317),\n",
       " (317, 318),\n",
       " (318, 319),\n",
       " (319, 320),\n",
       " (320, 321),\n",
       " (321, 325),\n",
       " (325, 326),\n",
       " (326, 327),\n",
       " (327, 328),\n",
       " (328, 329),\n",
       " (329, 330),\n",
       " (330, 331),\n",
       " (331, 332),\n",
       " (332, 333),\n",
       " (333, 334),\n",
       " (334, 335),\n",
       " (335, 336),\n",
       " (336, 337),\n",
       " (337, 338),\n",
       " (338, 339),\n",
       " (339, 340),\n",
       " (340, 341),\n",
       " (341, 342),\n",
       " (342, 343),\n",
       " (343, 344),\n",
       " (344, 345),\n",
       " (345, 346),\n",
       " (346, 347),\n",
       " (347, 348),\n",
       " (348, 349),\n",
       " (349, 350),\n",
       " (350, 351),\n",
       " (351, 352),\n",
       " (352, 353),\n",
       " (353, 354),\n",
       " (354, 355),\n",
       " (355, 356),\n",
       " (356, 360),\n",
       " (360, 361),\n",
       " (361, 362),\n",
       " (362, 363),\n",
       " (363, 364),\n",
       " (364, 365),\n",
       " (365, 366),\n",
       " (366, 367),\n",
       " (367, 368),\n",
       " (368, 369),\n",
       " (369, 370),\n",
       " (370, 371),\n",
       " (371, 372),\n",
       " (372, 373),\n",
       " (373, 374),\n",
       " (374, 375),\n",
       " (375, 376),\n",
       " (376, 377),\n",
       " (377, 378),\n",
       " (378, 379),\n",
       " (379, 380),\n",
       " (380, 381),\n",
       " (381, 382),\n",
       " (382, 383),\n",
       " (383, 384),\n",
       " (384, 385),\n",
       " (385, 386),\n",
       " (386, 387),\n",
       " (387, 388),\n",
       " (388, 390),\n",
       " (390, 391),\n",
       " (391, 392),\n",
       " (392, 393),\n",
       " (393, 394),\n",
       " (394, 395),\n",
       " (395, 396),\n",
       " (396, 397),\n",
       " (397, 398),\n",
       " (398, 399),\n",
       " (399, 400),\n",
       " (400, 401),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample['offset_mapping'][0]  #token对具体词的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49f4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_mapping = tokenized_sample.pop(\"offset_mapping\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad70d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sample.sequence_ids(0))  #每个token对应哪一个句子， None是[CLS]或[SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a09f19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['1963年'], 'answer_start': [30]} 30 35 17 382 47 48\n",
      "token answer: 1963 年\n",
      "{'text': ['1990年被擢升为天主教河内总教区宗座署理'], 'answer_start': [41]} 41 62 15 382 53 70\n",
      "token answer: 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理\n",
      "{'text': ['范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生'], 'answer_start': [97]} 97 126 15 382 100 124\n",
      "token answer: 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生\n",
      "{'text': ['1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理'], 'answer_start': [548]} 548 598 17 382 0 0\n",
      "token answer: [CLS]\n",
      "{'text': ['范廷颂于2009年2月22日清晨在河内离世'], 'answer_start': [759]} 759 780 12 382 0 0\n",
      "token answer: [CLS]\n",
      "{'text': ['《全美超级模特儿新秀大赛》第十季'], 'answer_start': [26]} 26 42 21 382 47 62\n",
      "token answer: 《 全 美 超 级 模 特 儿 新 秀 大 赛 》 第 十 季\n",
      "{'text': ['有前途的新面孔'], 'answer_start': [247]} 247 254 20 382 232 238\n",
      "token answer: 有 前 途 的 新 面 孔\n",
      "{'text': ['《Jet》、《东方日报》、《Elle》等'], 'answer_start': [706]} 706 726 20 382 0 0\n",
      "token answer: [CLS]\n",
      "{'text': ['售货员'], 'answer_start': [202]} 202 205 18 382 205 207\n",
      "token answer: 售 货 员\n",
      "{'text': ['大空翼'], 'answer_start': [84]} 84 87 21 382 105 107\n",
      "token answer: 大 空 翼\n"
     ]
    }
   ],
   "source": [
    "for idx, offset in enumerate(offsets_mapping):\n",
    "    answers = sample_dataset['answers'][idx]\n",
    "    start_char = answers['answer_start'][0]\n",
    "    end_char = start_char + len(answers['text'][0])\n",
    "    \n",
    "    context_start = tokenized_sample.sequence_ids(idx).index(1) #index获得list中第一个为1的索引 即context的开始索引\n",
    "    context_end   = tokenized_sample.sequence_ids(idx).index(None, context_start)-1 #从context_start开始，找到第一个为None的索引，减1得到context的结束索引\n",
    "    \n",
    "    # 判断答案是否超出了max_length；\n",
    "    if offset[context_end][1] < start_char or offset[context_start][0] > end_char: #判断答案是否超出了max_length；\n",
    "        start_token_pos = 0\n",
    "        end_token_pos = 0\n",
    "    else:\n",
    "        start_token_pos = context_start  \n",
    "        while start_token_pos < context_end and offset[start_token_pos][0] < start_char:\n",
    "            start_token_pos += 1\n",
    "        end_token_pos = context_end\n",
    "        while end_token_pos >  context_start and offset[end_token_pos][1] > end_char:\n",
    "            end_token_pos -= 1\n",
    "        \n",
    "    print(answers,start_char, end_char, context_start, context_end, start_token_pos, end_token_pos)\n",
    "    print('token answer:', tokenizer.decode(tokenized_sample['input_ids'][idx][start_token_pos:end_token_pos+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1d6919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    tokenized_dataset = tokenizer(\n",
    "        text = list(example['question']),\n",
    "        text_pair = list(example['context']),\n",
    "        return_offsets_mapping = True,\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        max_length = 384,\n",
    "    )\n",
    "    offset_mapping = tokenized_dataset.pop(\"offset_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for idx, offset in enumerate(offset_mapping):\n",
    "        answers = example['answers'][idx]\n",
    "        start_char = answers['answer_start'][0]\n",
    "        end_char   = start_char + len(answers['text'][0])\n",
    "\n",
    "        context_start = tokenized_dataset.sequence_ids(idx).index(1)\n",
    "        context_end   = tokenized_dataset.sequence_ids(idx).index(None, context_start)\n",
    "\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            start_token_pos = context_start\n",
    "            while start_token_pos < context_end and offset[start_token_pos][0] < start_char:\n",
    "                start_token_pos += 1\n",
    "            end_token_pos = context_end\n",
    "            while end_token_pos >  context_start and offset[end_token_pos][1] > end_char:\n",
    "                end_token_pos -= 1\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "    tokenized_dataset['start_positions'] = start_positions\n",
    "    tokenized_dataset['end_positions'] = end_positions\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af56610c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(process_func, batched=True, remove_columns=dataset['train'].column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad72d5a",
   "metadata": {},
   "source": [
    "## steps4 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e427726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained('hfl/chinese-macbert-base')\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1516062",
   "metadata": {},
   "source": [
    "## steps5 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4996915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='models_for_Question',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    run_name='runs',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7795d5",
   "metadata": {},
   "source": [
    "## steps6 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21413873",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bd36651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1902' max='1902' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1902/1902 07:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.831700</td>\n",
       "      <td>1.469791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.322200</td>\n",
       "      <td>1.171432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.296500</td>\n",
       "      <td>1.156372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1902, training_loss=1.99408299664719, metrics={'train_runtime': 451.7598, 'train_samples_per_second': 67.35, 'train_steps_per_second': 4.21, 'total_flos': 5962661340337152.0, 'train_loss': 1.99408299664719, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38818817",
   "metadata": {},
   "source": [
    "## steps7 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef266b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x1decbe7fca0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model,tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.1614495633693878e-05, 'start': 12, 'end': 13, 'answer': '常'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我的GPU训练不动全参数 只能微调分类头 结果就是很差啊\n",
    "pipe(question=\"如何评价《肖申克的救赎》？\", context=\"《肖申克的救赎》是一个非常棒的电影\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
